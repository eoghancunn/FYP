{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInital evaluation of active learning - compare classification on initial (core/bootstrap) set before and \\nafter the addition of egal queried samples to the training set.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inital evaluation of active learning - compare classification on initial (core/bootstrap) set before and \n",
    "after the addition of egal queried samples to the training set.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import json \n",
    "import seaborn as sns\n",
    "import dns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb+srv://eoghan:Ailbhe123@fypcluster-cqcwt.mongodb.net/test?retryWrites=true&w=majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.beta_db\n",
    "comments = db.comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = list(comments.find({'$or':[{'unseen_test' : {'$exists': 'true'}},\n",
    "                                          {'final_test' : {'$exists': 'true'}},\n",
    "                                          {\"label\" : {\"$exists\": 'true'}},\n",
    "                                          {\"queried\" : 1}]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_grades_dict = {'M': 0,'D': 0,'HD' : 0,'VD' : 0,'HVD' : 0,'MS' : 0, \n",
    "                       'S' : 1,'HS' : 2,'MVS' : 3,'VS' : 4,'HVS' : 5, 'ED1' : 5,\n",
    "                       'E1' : 6,'E2' : 7,'E3' : 8,'E4' : 9,'E5' : 10,\n",
    "                       'E6' : 11,'E7' : 12,'E8' : 13,'E9' : 14,'E10' : 14,\n",
    "                       'E11' : 15, 'XS': 16, 'HXS' : 17, 'none': np.nan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['max_grade_of_climber'] = df['max_grade of climber'].map(lambda x: british_grades_dict[x] if type(x) is str else x)\n",
    "df['route_grade'] = df['route_grade'].map(lambda x: british_grades_dict[x] if type(x) is str else x)\n",
    "df['challenge'] = df['route_grade'] - df['max_grade_of_climber']\n",
    "df['challenge'] = df['challenge'].fillna(df.challenge.mean())\n",
    "df['is_local'] = (df['local_to'] == df['location']).map(lambda x : 1 if x else 0)\n",
    "df['comment_len'] = df.comment.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 35)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queried_df = df[~df['queried'].isna()]\n",
    "queried_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 35)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = df[(~df['final_test'].isna()) | (~df['unseen_test'].isna())]\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 35)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_df =  df[~df['label'].isna()]\n",
    "init_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried_df = queried_df.drop(['annotator_2', 'annotator_7'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried_df['annotation'] = (queried_df[[col for col in queried_df.columns if 'annotator' in col]].mean(axis = 1)+0.01).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried_df['annotation'] = queried_df['annotation'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    62\n",
       "1    38\n",
       "Name: annotation, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queried_df['annotation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eoghancunningham/anaconda3/envs/datascinp/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "init_df['label'] = (init_df[['label','pilot_1','pilot_2','pilot_3']].mean(axis = 1)+0.01).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    171\n",
       "1.0    133\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eoghancunningham/anaconda3/envs/datascinp/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['annotation'] = (test_df[['annotator_3','me','annotator_1']].mean(axis = 1)+0.01).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        words = word_tokenize(doc)\n",
    "        new_words= [word for word in words if word.isalnum()]\n",
    "        return [self.wnl.lemmatize(t) for t in new_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range = (1,3), min_df = 5, stop_words = 'english',\n",
    "                            tokenizer = LemmaTokenizer())\n",
    "clf = RandomForestClassifier(n_estimators = 500, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = list(powerset(['comment_len','is_local','challenge']))[1:]\n",
    "feature_sets = list(map(list,feature_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "                        feature\\_set &   auc &   bas \\\\\n",
      "\\midrule\n",
      "                      [comment\\_len] & 0.850 & 0.795 \\\\\n",
      "                         [is\\_local] & 0.843 & 0.765 \\\\\n",
      "                        [challenge] & 0.844 & 0.772 \\\\\n",
      "            [comment\\_len, is\\_local] & 0.851 & 0.780 \\\\\n",
      "           [comment\\_len, challenge] & 0.849 & 0.781 \\\\\n",
      "              [is\\_local, challenge] & 0.839 & 0.765 \\\\\n",
      " [comment\\_len, is\\_local, challenge] & 0.845 & 0.760 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "performance of additional features post active learning (with AL training samples) : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_set</th>\n",
       "      <th>auc</th>\n",
       "      <th>bas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[comment_len]</td>\n",
       "      <td>0.850315</td>\n",
       "      <td>0.794523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[is_local]</td>\n",
       "      <td>0.842514</td>\n",
       "      <td>0.764618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[challenge]</td>\n",
       "      <td>0.843538</td>\n",
       "      <td>0.771749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[comment_len, is_local]</td>\n",
       "      <td>0.850867</td>\n",
       "      <td>0.780260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[comment_len, challenge]</td>\n",
       "      <td>0.849291</td>\n",
       "      <td>0.781009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[is_local, challenge]</td>\n",
       "      <td>0.838731</td>\n",
       "      <td>0.764618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[comment_len, is_local, challenge]</td>\n",
       "      <td>0.844878</td>\n",
       "      <td>0.760362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          feature_set       auc       bas\n",
       "0                       [comment_len]  0.850315  0.794523\n",
       "1                          [is_local]  0.842514  0.764618\n",
       "2                         [challenge]  0.843538  0.771749\n",
       "3             [comment_len, is_local]  0.850867  0.780260\n",
       "4            [comment_len, challenge]  0.849291  0.781009\n",
       "5               [is_local, challenge]  0.838731  0.764618\n",
       "6  [comment_len, is_local, challenge]  0.844878  0.760362"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = init_df.comment.append(queried_df.comment, ignore_index = True)\n",
    "y_train = init_df.label.append(queried_df.annotation, ignore_index = True)\n",
    "\n",
    "X_test = test_df.comment\n",
    "y_test = test_df.annotation\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature_set in feature_sets:\n",
    "\n",
    "    X_train_vect = tfidf_vect.fit_transform(X_train)\n",
    "    X_test_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "    additional_train = np.vstack((init_df[feature_set].values,\n",
    "                                 queried_df[feature_set].values))\n",
    "\n",
    "    X_train_vect = sparse.hstack((X_train_vect,additional_train))\n",
    "    X_test_vect = sparse.hstack((X_test_vect,test_df[feature_set].values))\n",
    "\n",
    "    clf.fit(X_train_vect.toarray(), y_train)\n",
    "    predicted_proba = clf.predict_proba(X_test_vect.toarray())\n",
    "    pred = (predicted_proba [:,1] >= 0.44).astype('int')\n",
    "\n",
    "    bas = metrics.balanced_accuracy_score(y_test, pred)\n",
    "    roc = metrics.roc_curve(y_test,predicted_proba [:,1])\n",
    "    auc = metrics.auc(x = roc[0], y = roc[1])\n",
    "    \n",
    "    results.append({'feature_set': feature_set, 'bas': bas, 'auc': auc})\n",
    "\n",
    "performance = pd.DataFrame(results)[['feature_set', 'auc', 'bas']]\n",
    "print(performance.to_latex(index = False,float_format = \"%.3f\"))\n",
    "print('performance of additional features post active learning (with AL training samples) : \\n')\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.80      0.86       235\n",
      "         1.0       0.45      0.70      0.55        54\n",
      "\n",
      "    accuracy                           0.79       289\n",
      "   macro avg       0.69      0.75      0.70       289\n",
      "weighted avg       0.83      0.79      0.80       289\n",
      "\n",
      "0.7539795114263199\n",
      "0.837431048069346\n"
     ]
    }
   ],
   "source": [
    "X_train = init_df.comment.append(queried_df.comment, ignore_index = True)\n",
    "y_train = init_df.label.append(queried_df.annotation, ignore_index = True)\n",
    "\n",
    "X_test = test_df.comment\n",
    "y_test = test_df.annotation\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train_vect = tfidf_vect.fit_transform(X_train)\n",
    "X_test_vect = tfidf_vect.transform(X_test)\n",
    "    \n",
    "clf.fit(X_train_vect.toarray(), y_train)\n",
    "predicted_proba = clf.predict_proba(X_test_vect.toarray())\n",
    "pred = (predicted_proba [:,1] >= 0.44).astype('int')\n",
    "\n",
    "print(metrics.classification_report(y_test, pred))\n",
    "print(metrics.balanced_accuracy_score(y_test, pred))\n",
    "roc = metrics.roc_curve(y_test,predicted_proba [:,1])\n",
    "print(metrics.auc(x = roc[0], y = roc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.69      0.80       235\n",
      "         1.0       0.37      0.78      0.50        54\n",
      "\n",
      "    accuracy                           0.71       289\n",
      "   macro avg       0.65      0.74      0.65       289\n",
      "weighted avg       0.83      0.71      0.74       289\n",
      "\n",
      "0.7356973995271867\n",
      "0.8295902285263989\n"
     ]
    }
   ],
   "source": [
    "X_train = init_df.comment\n",
    "y_train = init_df.label\n",
    "\n",
    "X_test = test_df.comment\n",
    "y_test = test_df.annotation\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "X_train_vect = tfidf_vect.fit_transform(X_train)\n",
    "X_test_vect = tfidf_vect.transform(X_test)\n",
    "    \n",
    "clf.fit(X_train_vect.toarray(), y_train)\n",
    "predicted_proba = clf.predict_proba(X_test_vect.toarray())\n",
    "pred = (predicted_proba [:,1] >= 0.38).astype('int')\n",
    "\n",
    "print(metrics.classification_report(y_test, pred))\n",
    "print(metrics.balanced_accuracy_score(y_test, pred))\n",
    "roc = metrics.roc_curve(y_test,predicted_proba [:,1])\n",
    "print(metrics.auc(x = roc[0], y = roc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "                        feature\\_set &   auc &   bas \\\\\n",
      "\\midrule\n",
      "                      [comment\\_len] & 0.839 & 0.745 \\\\\n",
      "                         [is\\_local] & 0.829 & 0.725 \\\\\n",
      "                        [challenge] & 0.830 & 0.731 \\\\\n",
      "            [comment\\_len, is\\_local] & 0.837 & 0.754 \\\\\n",
      "           [comment\\_len, challenge] & 0.839 & 0.750 \\\\\n",
      "              [is\\_local, challenge] & 0.827 & 0.731 \\\\\n",
      " [comment\\_len, is\\_local, challenge] & 0.834 & 0.741 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "performance of additional features without AL training samples : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_set</th>\n",
       "      <th>auc</th>\n",
       "      <th>bas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[comment_len]</td>\n",
       "      <td>0.838968</td>\n",
       "      <td>0.744957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[is_local]</td>\n",
       "      <td>0.828999</td>\n",
       "      <td>0.725059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[challenge]</td>\n",
       "      <td>0.829748</td>\n",
       "      <td>0.731442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[comment_len, is_local]</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>0.754216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[comment_len, challenge]</td>\n",
       "      <td>0.838731</td>\n",
       "      <td>0.749961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[is_local, challenge]</td>\n",
       "      <td>0.827108</td>\n",
       "      <td>0.731442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[comment_len, is_local, challenge]</td>\n",
       "      <td>0.834397</td>\n",
       "      <td>0.740701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          feature_set       auc       bas\n",
       "0                       [comment_len]  0.838968  0.744957\n",
       "1                          [is_local]  0.828999  0.725059\n",
       "2                         [challenge]  0.829748  0.731442\n",
       "3             [comment_len, is_local]  0.837155  0.754216\n",
       "4            [comment_len, challenge]  0.838731  0.749961\n",
       "5               [is_local, challenge]  0.827108  0.731442\n",
       "6  [comment_len, is_local, challenge]  0.834397  0.740701"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = init_df.comment\n",
    "y_train = init_df.label\n",
    "\n",
    "X_test = test_df.comment\n",
    "y_test = test_df.annotation\n",
    "\n",
    "results = []\n",
    "\n",
    "for feature_set in feature_sets:\n",
    "\n",
    "    X_train_vect = tfidf_vect.fit_transform(X_train)\n",
    "    X_test_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "    additional_train = np.vstack((init_df[feature_set].values))\n",
    "\n",
    "    X_train_vect = sparse.hstack((X_train_vect,additional_train))\n",
    "    X_test_vect = sparse.hstack((X_test_vect,test_df[feature_set].values))\n",
    "\n",
    "    clf.fit(X_train_vect.toarray(), y_train)\n",
    "    predicted_proba = clf.predict_proba(X_test_vect.toarray())\n",
    "    pred = (predicted_proba [:,1] >= 0.38).astype('int')\n",
    "\n",
    "    bas = metrics.balanced_accuracy_score(y_test, pred)\n",
    "    roc = metrics.roc_curve(y_test,predicted_proba [:,1])\n",
    "    auc = metrics.auc(x = roc[0], y = roc[1])\n",
    "    \n",
    "    results.append({'feature_set': feature_set, 'bas': bas, 'auc': auc})\n",
    "performance = pd.DataFrame(results)[['feature_set', 'auc', 'bas']]\n",
    "print(performance.to_latex(index = False,float_format = \"%.3f\"))\n",
    "print('performance of additional features pre active learning (without AL training samples) : \\n')\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115394, 31)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all comments \n",
    "df_total = pd.DataFrame(comments.find())\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute comment_len\n",
    "df_total['comment_len'] = df_total.comment.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorising\n",
      "vectorised\n",
      "fitting\n",
      "predicting\n"
     ]
    }
   ],
   "source": [
    "# classify all comments with best performing model\n",
    "X_train = init_df.comment.append(queried_df.comment, ignore_index = True)\n",
    "y_train = init_df.label.append(queried_df.annotation, ignore_index = True)\n",
    "\n",
    "X_test = df_total.comment\n",
    "\n",
    "X_train_vect = tfidf_vect.fit_transform(X_train)\n",
    "print('vectorising')\n",
    "X_test_vect = tfidf_vect.transform(X_test)\n",
    "print('vectorised')\n",
    "\n",
    "additional_train = np.vstack((init_df[['comment_len']].values,\n",
    "                                 queried_df[['comment_len']].values))\n",
    "\n",
    "X_train_vect = sparse.hstack((X_train_vect,additional_train))\n",
    "X_test_vect = sparse.hstack((X_test_vect,df_total[['comment_len']].values))\n",
    "\n",
    "print('fitting')\n",
    "clf.fit(X_train_vect.toarray(), y_train)\n",
    "print('predicting')\n",
    "predicted_proba = clf.predict_proba(X_test_vect.toarray())\n",
    "pred = (predicted_proba [:,1] >= 0.44).astype('int')\n",
    "\n",
    "pred = pd.Series(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10582872593029101"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best estimate of positive class rate in our dataset. \n",
    "val_counts[1]/(val_counts[0]+val_counts[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascinp)",
   "language": "python",
   "name": "datascinp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
